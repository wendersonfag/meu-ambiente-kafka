# ü•â Consumer Bronze - Guia de Uso

## üìå Vis√£o Geral

O **Consumer Bronze** √© um notebook Databricks que:
- ‚úÖ L√™ dados **em tempo real** dos 3 t√≥picos Kafka (`ordem`, `eventos`, `gps`)
- ‚úÖ Captura dados brutos com **metadados Kafka** (key, value, partition, offset, timestamp)
- ‚úÖ Cria **3 tabelas Bronze** no Delta Lake
- ‚úÖ Processa com **trigger de 5 segundos**
- ‚úÖ Estrutura id√™ntica √† imagem de refer√™ncia

## üìÇ Arquivo

```
databricks-notebooks/databricks-bronze-consumer.py
```

## üöÄ Como Usar

### 1. Pr√©-requisitos

Certifique-se de que:

```powershell
# Terminal 1: Inicie o ambiente Kafka
.\start-kafka-env.ps1

# Aguarde at√© ver:
# ‚úÖ Kafka rodando
# ‚úÖ Producer gerando dados
# ‚úÖ Bore URL dispon√≠vel
```

### 2. Configurar Kafka Broker (se usando Bore)

Se estiver usando **Bore tunnel** para acessar de Databricks remotamente:

```powershell
# O script start-kafka-env.ps1 j√° cria bore_url.txt
# Verifique o conte√∫do:
Get-Content bore_url.txt
# Sa√≠da exemplo: bore.pub:53049
```

### 3. Executar no Databricks

1. Abra o Databricks workspace
2. V√° para: **Workspace > Repos > seu-projeto**
3. Abra o notebook: `databricks-bronze-consumer`
4. **Execute todas as c√©lulas** clicando no bot√£o ‚ñ∂Ô∏è **Run All**

## üìä Estrutura das Tabelas Bronze

Cada tabela Bronze tem a seguinte estrutura:

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `key` | STRING | Chave da mensagem Kafka |
| `value` | STRING | Payload JSON da mensagem |
| `partition` | INTEGER | Parti√ß√£o Kafka |
| `offset` | LONG | Offset da mensagem |
| `timestamp` | TIMESTAMP | Timestamp da mensagem Kafka |
| `timestampType` | INTEGER | Tipo de timestamp Kafka |
| `topic` | STRING | Nome do t√≥pico |
| `bronze_ingestion_timestamp` | TIMESTAMP | Timestamp de ingest√£o no Bronze |

### Tabelas Criadas

```sql
-- Tabela 1: Pedidos/Pagamentos
SELECT * FROM kafka_bronze_ordem

-- Tabela 2: Eventos de Pagamento
SELECT * FROM kafka_bronze_eventos

-- Tabela 3: Dados de GPS
SELECT * FROM kafka_bronze_gps
```

## üîÑ Como Funciona

### 1. Leitura Kafka
```python
spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", KAFKA_BROKER)
  .option("subscribe", "ordem|eventos|gps")
  .load()
```

### 2. Captura de Metadados
```python
df.select(
    "key",           # Chave da mensagem
    "value",         # Valor/payload
    "partition",     # Parti√ß√£o Kafka
    "offset",        # Offset
    "timestamp",     # Timestamp Kafka
    "timestampType"  # Tipo de timestamp
)
```

### 3. Escrita em Delta
```python
df.writeStream
  .format("delta")
  .mode("append")
  .trigger(processingTime="5 seconds")
  .table("kafka_bronze_ordem")
```

## üíæ Configura√ß√µes

No notebook, voc√™ pode ajustar:

```python
# Broker Kafka
KAFKA_BROKER = "bore.pub:53049"  # ou "localhost:29092" para local

# Intervalo de processamento (trigger)
TRIGGER_INTERVAL = "5 seconds"  # ou "10 seconds", "1 minute"

# Checkpoint (para rastrear offsets processados)
CHECKPOINT_BASE = "/tmp/kafka-bronze-checkpoints"
```

## üîç Monitorar os Streams

### Visualizar Dados Bronze

O notebook exibe automaticamente:
- ‚úÖ √öltimas 100 linhas de cada tabela
- ‚úÖ Contagem de linhas por tabela
- ‚úÖ Status dos streams ativos

### Verificar Streams Ativos

```python
# C√©lula 11 do notebook mostra:
spark.streams.active  # Lista todos os streams rodando
```

### Query SQL para Explorar

```sql
-- Ver dados mais recentes
SELECT * FROM kafka_bronze_ordem
ORDER BY bronze_ingestion_timestamp DESC
LIMIT 10

-- Contar por parti√ß√£o
SELECT partition, COUNT(*) as count
FROM kafka_bronze_ordem
GROUP BY partition

-- Verificar offsets
SELECT partition, MAX(offset) as max_offset
FROM kafka_bronze_ordem
GROUP BY partition ORDER BY partition
```

## üõë Parar os Streams

Para parar **sem perder dados**:

1. **Op√ß√£o 1**: Descomente a c√©lula 12 e execute:
```python
for stream in spark.streams.active:
    stream.stop()
```

2. **Op√ß√£o 2**: Clique em **Cancel** no Databricks

Os dados j√° salvos permanecer√£o nas tabelas Bronze.

## üóëÔ∏è Resetar e Reprocessar

Para **limpar checkpoints** e reprocessar tudo do in√≠cio:

1. Descomente a c√©lula 13:
```python
dbutils.fs.rm(CHECKPOINT_BASE, recurse=True)
```

2. Execute novamente o notebook

‚ö†Ô∏è **Aviso**: Isso **n√£o apaga** a tabela Bronze, apenas reprocessa desde o in√≠cio dos t√≥picos Kafka.

## üìà Pr√≥ximos Passos

### Criar Camada Silver

Depois que os dados est√£o na Bronze, voc√™ pode criar a **Silver** com dados limpos:

```python
# Exemplo: Parsear JSON de ordem
df_silver_ordem = spark.sql("""
    SELECT
        topic,
        partition,
        offset,
        timestamp,
        from_json(value, 'payment_id STRING, order_key STRING, amount DOUBLE, ...') as data
    FROM kafka_bronze_ordem
    WHERE value IS NOT NULL
""")
```

### Criar Camada Gold

Depois da Silver, criar **Gold** com agrega√ß√µes:

```python
# Exemplo: Resumo por status
df_gold = spark.sql("""
    SELECT
        from_json(value, schema).data.status,
        COUNT(*) as count,
        SUM(from_json(value, schema).data.amount) as total_amount
    FROM kafka_bronze_ordem
    GROUP BY status
""")
```

## üîß Troubleshooting

### Problema: "N√£o consegue conectar ao Kafka"
- ‚úÖ Verifique se `start-kafka-env.ps1` foi executado
- ‚úÖ Verifique o `bore_url.txt` tem uma URL v√°lida
- ‚úÖ Teste: `nc -zv bore.pub xxxxx` (no seu terminal)

### Problema: "Tabela n√£o encontrada"
- ‚úÖ As tabelas s√£o criadas automaticamente na primeira execu√ß√£o
- ‚úÖ Aguarde a c√©lula 5 completar
- ‚úÖ Verifique: `SHOW TABLES LIKE 'kafka_bronze%'`

### Problema: "Sem dados nas tabelas"
- ‚úÖ Verifique se o producer est√° rodando: `docker-compose logs python-producer`
- ‚úÖ Aguarde alguns segundos (trigger √© 5s)
- ‚úÖ Verifique t√≥picos: `docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list`

### Problema: "Checkpoint location n√£o acess√≠vel"
- ‚úÖ Se usar DBFS, ajuste em Databricks para `/dbfs/tmp/kafka-bronze-checkpoints`
- ‚úÖ Ou use Unity Catalog: `/Volumes/main/default/checkpoint`

## üìö Estrutura Completa do Projeto

```
meu-ambiente-kafka/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îî‚îÄ‚îÄ producer.py
‚îú‚îÄ‚îÄ spark-consumer/
‚îÇ   ‚îî‚îÄ‚îÄ consumer.py (local)
‚îú‚îÄ‚îÄ databricks-notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ databricks-generator.py (simula dados)
‚îÇ   ‚îú‚îÄ‚îÄ databricks-consumer.py (Auto Loader)
‚îÇ   ‚îî‚îÄ‚îÄ databricks-bronze-consumer.py ‚≠ê (novo - Kafka em tempo real)
‚îú‚îÄ‚îÄ start-kafka-env.ps1
‚îî‚îÄ‚îÄ CLAUDE.md

```

## üí° Dicas Importantes

1. **Trigger de 5 segundos**: Significa que a cada 5 segundos, novos dados s√£o processados
2. **Modo Append**: Dados novos s√£o **adicionados**, nunca sobrescrevem
3. **Checkpoints**: Garantem que cada mensagem √© processada exatamente uma vez
4. **Metadados Kafka**: Preservados para auditoria e rastreamento

## üéØ Exemplo Completo de Uso

```bash
# Terminal 1: Inicie Kafka
.\start-kafka-env.ps1

# Aguarde mensagens:
# ‚úÖ Producer gerando dados
# ‚úÖ Bore URL criada

# Terminal 2 (Databricks): Execute o notebook
# 1. Abra databricks-bronze-consumer.py
# 2. Clique "Run All"
# 3. Aguarde ~30 segundos

# Terminal 2: Verifique os dados
SELECT COUNT(*) FROM kafka_bronze_ordem  -- Crescendo a cada 5s

# Terminal 1: Parar quando finalizado
Ctrl+C
docker-compose down
```

## üìû Suporte

Para d√∫vidas, verifique:
- ‚úÖ `CLAUDE.md` - Documenta√ß√£o geral
- ‚úÖ `TESTE-DATABRICKS.md` - Testes espec√≠ficos
- ‚úÖ Spark UI: http://localhost:4040 (quando rodando localmente)
- ‚úÖ Databricks: Clusters > Your Cluster > Apps

---

**√öltima Atualiza√ß√£o**: 2025-11-07
**Vers√£o**: 1.0
**Spark**: 3.5.0+
**Databricks Runtime**: 13.3+ (com Kafka suportado)
