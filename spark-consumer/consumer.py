#!/usr/bin/env python3
"""
Spark Streaming Consumer - Consome dados de 3 t√≥picos Kafka
T√≥picos: ordem, eventos, gps
"""

import os
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType

# Configura√ß√µes
KAFKA_BROKER = os.getenv('KAFKA_BROKER', 'kafka:9092')
TOPIC_ORDER = 'ordem'
TOPIC_EVENTOS = 'eventos'
TOPIC_GPS = 'gps'

def wait_for_kafka(broker, max_retries=30):
    """Aguarda o Kafka estar pronto"""
    print(f"Aguardando Kafka em {broker}...")
    for i in range(max_retries):
        try:
            # Tenta criar uma sess√£o simples para testar conex√£o
            from socket import socket, AF_INET, SOCK_STREAM
            host, port = broker.split(':')
            sock = socket(AF_INET, SOCK_STREAM)
            sock.settimeout(2)
            result = sock.connect_ex((host, int(port)))
            sock.close()
            if result == 0:
                print("Kafka est√° pronto!")
                time.sleep(5)  # Aguarda mais um pouco para garantir
                return True
        except Exception as e:
            print(f"Tentativa {i+1}/{max_retries}: Kafka n√£o est√° pronto ({e})")
        time.sleep(2)
    raise Exception("Kafka n√£o ficou dispon√≠vel a tempo")

def create_spark_session():
    """Cria a SparkSession com configura√ß√µes para Kafka"""
    spark = SparkSession.builder \
        .appName("KafkaSparkConsumer") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
        .config("spark.streaming.stopGracefullyOnShutdown", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

# --- Schemas dos Dados ---

# Schema para o t√≥pico 'ordem'
order_data_schema = StructType([
    StructField("payment_id", StringType(), True),
    StructField("order_key", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("currency", StringType(), True),
    StructField("method", StringType(), True),
    StructField("status", StringType(), True),
    StructField("card_brand", StringType(), True),
    StructField("card_last4", StringType(), True),
    StructField("net_amount", DoubleType(), True),
    StructField("country", StringType(), True),
    StructField("ip_address", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("dt_current_timestamp", StringType(), True)
])

order_schema = StructType([
    StructField("data", order_data_schema, True)
])

# Schema para o t√≥pico 'eventos'
event_details_schema = StructType([
    StructField("event_name", StringType(), True),
    StructField("timestamp", StringType(), True)
])

event_data_schema = StructType([
    StructField("event_id", StringType(), True),
    StructField("payment_id", StringType(), True),
    StructField("event", event_details_schema, True),
    StructField("dt_current_timestamp", StringType(), True)
])

event_schema = StructType([
    StructField("data", event_data_schema, True)
])

# Schema para o t√≥pico 'gps'
gps_data_schema = StructType([
    StructField("gps_id", StringType(), True),
    StructField("order_id", StringType(), True),
    StructField("lat", DoubleType(), True),
    StructField("lon", DoubleType(), True),
    StructField("speed_kph", IntegerType(), True),
    StructField("accuracy_m", DoubleType(), True),
    StructField("timestamp", StringType(), True),
    StructField("dt_current_timestamp", StringType(), True)
])

gps_schema = StructType([
    StructField("data", gps_data_schema, True)
])

def consume_topic(spark, topic_name, schema, query_name):
    """
    Consome dados de um t√≥pico Kafka espec√≠fico
    
    Args:
        spark: SparkSession
        topic_name: Nome do t√≥pico Kafka
        schema: Schema dos dados JSON
        query_name: Nome da query para identifica√ß√£o
    """
    print(f"\n{'='*60}")
    print(f"Iniciando consumo do t√≥pico: {topic_name}")
    print(f"{'='*60}\n")
    
    # L√™ o stream do Kafka
    df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BROKER) \
        .option("subscribe", topic_name) \
        .option("startingOffsets", "earliest") \
        .option("failOnDataLoss", "false") \
        .load()
    
    # Converte o valor de bytes para string e depois para JSON
    parsed_df = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)", "timestamp") \
        .select(
            col("key"),
            from_json(col("value"), schema).alias("json_data"),
            col("timestamp").alias("kafka_timestamp")
        ) \
        .select("key", "json_data.data.*", "kafka_timestamp")
    
    # Mostra o schema
    print(f"\nüìã Schema do t√≥pico '{topic_name}':")
    parsed_df.printSchema()
    
    # Escreve no console
    query = parsed_df \
        .writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .queryName(query_name) \
        .trigger(processingTime="5 seconds") \
        .start()
    
    return query

def main():
    """Fun√ß√£o principal"""
    print("="*60)
    print("üöÄ Iniciando Spark Streaming Consumer")
    print("="*60)
    
    # Aguarda o Kafka
    wait_for_kafka(KAFKA_BROKER)
    
    # Cria a Spark Session
    spark = create_spark_session()
    print(f"‚úÖ SparkSession criada: {spark.version}")
    
    try:
        # Inicia o consumo dos tr√™s t√≥picos
        query_ordem = consume_topic(spark, TOPIC_ORDER, order_schema, "query_ordem")
        query_eventos = consume_topic(spark, TOPIC_EVENTOS, event_schema, "query_eventos")
        query_gps = consume_topic(spark, TOPIC_GPS, gps_schema, "query_gps")
        
        print("\n" + "="*60)
        print("‚úÖ Todos os consumers est√£o rodando!")
        print("üìä Acesse a Spark UI em: http://localhost:4040")
        print("‚èπÔ∏è  Pressione CTRL+C para parar")
        print("="*60 + "\n")
        
        # Aguarda todas as queries
        query_ordem.awaitTermination()
        query_eventos.awaitTermination()
        query_gps.awaitTermination()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Parando consumer...")
    except Exception as e:
        print(f"\n‚ùå Erro: {e}")
    finally:
        spark.stop()
        print("‚úÖ Spark Session encerrada")

if __name__ == "__main__":
    main()