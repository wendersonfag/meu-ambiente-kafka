# Databricks Kafka Simulator - Medallion Architecture

Este simulador permite testar e estudar Spark Structured Streaming no Databricks **sem precisar de conexÃ£o Kafka** ou tunelamento via Bore.

Implementa a **Arquitetura Medallion** (Bronze-Silver-Gold) com ingestion em tempo real na camada Bronze.

## VisÃ£o Geral

O simulador consiste em 2 notebooks Python que replicam o comportamento do ambiente Kafka local:

1. **databricks-generator.py** - Gera arquivos JSON simulando os 3 tÃ³picos Kafka com sequence_id para ordem
2. **databricks-consumer.py** - Consome esses arquivos com Auto Loader e grava na camada **Bronze** em Delta Lake

## Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATABRICKS - MEDALLION ARCH                      â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Generator      â”‚         â”‚   Consumer   â”‚      â”‚   Bronze    â”‚ â”‚
â”‚  â”‚   Notebook       â”‚         â”‚   Notebook   â”‚      â”‚   (Delta)   â”‚ â”‚
â”‚  â”‚                  â”‚         â”‚              â”‚      â”‚             â”‚ â”‚
â”‚  â”‚  Gera JSONs      â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Auto Loader  â”‚â”€â”€â”€â”€â”€>â”‚ Tabela      â”‚ â”‚
â”‚  â”‚  + sequence_id   â”‚  DBFS   â”‚ (readStream) â”‚      â”‚ kafka_      â”‚ â”‚
â”‚  â”‚  a cada 5s       â”‚         â”‚              â”‚      â”‚ multiplex   â”‚ â”‚
â”‚  â”‚                  â”‚         â”‚ Transforma:  â”‚      â”‚             â”‚ â”‚
â”‚  â”‚  â€¢ kafka-orders  â”‚         â”‚ â€¢ key        â”‚      â”‚ key         â”‚ â”‚
â”‚  â”‚  â€¢ kafka-events  â”‚         â”‚ â€¢ value      â”‚      â”‚ value       â”‚ â”‚
â”‚  â”‚  â€¢ kafka-gps     â”‚         â”‚ â€¢ topic      â”‚      â”‚ topic       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â€¢ partition  â”‚      â”‚ partition   â”‚ â”‚
â”‚                               â”‚ â€¢ offset     â”‚      â”‚ offset      â”‚ â”‚
â”‚  ğŸ“‚ /FileStore/kafka-sim/      â”‚ â€¢ timestamp  â”‚      â”‚ timestamp   â”‚ â”‚
â”‚     â”œâ”€â”€ kafka-orders/          â”‚ â€¢ ingestion  â”‚      â”‚ ingestion_  â”‚ â”‚
â”‚     â”œâ”€â”€ kafka-events/          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ timestamp   â”‚ â”‚
â”‚     â””â”€â”€ kafka-gps/                                   â”‚             â”‚ â”‚
â”‚                                                      â”‚ Trigger: 5s â”‚ â”‚
â”‚                                   ğŸ“ Checkpoint:     â”‚ Formato:    â”‚ â”‚
â”‚                            /tmp/kafka_multiplex     â”‚ Delta       â”‚ â”‚
â”‚                            _checkpoint              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Estrutura dos Dados

Os dados gerados sÃ£o **idÃªnticos** ao ambiente Kafka local, mantendo a mesma estrutura JSON:

### TÃ³pico: kafka-orders

```json
{
  "data": {
    "_sequence_id": 1,
    "payment_id": "uuid",
    "order_key": "uuid",
    "amount": 123.45,
    "currency": "BRL",
    "method": "Card",
    "status": "succeeded",
    "card_brand": "Visa",
    "card_last4": "1234",
    "net_amount": 111.10,
    "country": "BR",
    "ip_address": "192.168.1.1",
    "timestamp": "2025-01-15 10:30:45.123",
    "dt_current_timestamp": "2025-01-15 10:30:45.123"
  }
}
```

### TÃ³pico: kafka-events

```json
{
  "data": {
    "event_id": "uuid",
    "payment_id": "uuid",  // Relacionado com ordem
    "event": {
      "event_name": "authorized",
      "timestamp": "2025-01-15 10:30:45.123"
    },
    "dt_current_timestamp": "2025-01-15 10:30:45.123"
  }
}
```

### TÃ³pico: kafka-gps

```json
{
  "data": {
    "gps_id": "uuid",
    "order_id": "uuid",  // Relacionado com ordem (order_key)
    "lat": -23.5505,
    "lon": -46.6333,
    "speed_kph": 45,
    "accuracy_m": 15.5,
    "timestamp": "2025-01-15 10:30:45.123",
    "dt_current_timestamp": "2025-01-15 10:30:45.123"
  }
}
```

## Estrutura da Camada Bronze (Hive Metastore)

Os dados sÃ£o transformados e salvos em Delta Lake na tabela `bronze.kafka_multiplex`:

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| **key** | LONG | Ordem sequencial de entrada (_sequence_id) |
| **value** | STRING | JSON completo do evento (convertido para string) |
| **topic** | STRING | Nome do tÃ³pico (kafka-orders, kafka-events, kafka-gps) |
| **partition** | INT | PartiÃ§Ã£o simulada (0-2, baseada em hash da key) |
| **offset** | LONG | Offset sequencial dentro da partiÃ§Ã£o (comeÃ§a em 0) |
| **timestamp** | TIMESTAMP | Timestamp do dado original |
| **ingestion_timestamp** | TIMESTAMP | Quando o dado foi inserido na Bronze (current_timestamp) |

**Exemplo de dados na Bronze:**

```
key=1, topic="kafka-orders", partition=0, offset=0, value={"_sequence_id": 1, "payment_id": "...", ...}
key=2, topic="kafka-events", partition=1, offset=0, value={"_sequence_id": 2, "event_id": "...", ...}
key=3, topic="kafka-gps", partition=2, offset=0, value={"_sequence_id": 3, "gps_id": "...", ...}
...
```

**LocalizaÃ§Ã£o no Hive Metastore:**
- Database: `bronze`
- Tabela: `kafka_multiplex`
- Checkpoint: `/tmp/kafka_multiplex_checkpoint`

## Guia de Uso Passo a Passo

### 1. Preparar o Ambiente Databricks

#### OpÃ§Ã£o A: Databricks Community Edition (Gratuito)

1. Acesse: https://community.cloud.databricks.com/
2. Crie uma conta gratuita (se ainda nÃ£o tiver)
3. FaÃ§a login

#### OpÃ§Ã£o B: Databricks Workspace (Pago/Trial)

1. Acesse seu workspace
2. Certifique-se de ter um cluster disponÃ­vel

### 2. Importar os Notebooks

#### Via Upload de Arquivos:

1. No Databricks, vÃ¡ para **Workspace** (barra lateral)
2. Clique com botÃ£o direito na pasta desejada â†’ **Import**
3. Escolha **File** e faÃ§a upload de:
   - `databricks-generator.py`
   - `databricks-consumer.py`

#### Via Git (Recomendado):

1. No Databricks, vÃ¡ para **Repos**
2. Clique em **Add Repo**
3. Cole a URL do seu repositÃ³rio: `https://github.com/wendersonfag/meu-ambiente-kafka`
4. Os notebooks estarÃ£o em: `databricks-notebooks/`

### 3. Criar/Iniciar um Cluster

1. No Databricks, vÃ¡ para **Compute** (barra lateral)
2. Se nÃ£o tiver cluster:
   - Clique em **Create Cluster**
   - Escolha configuraÃ§Ãµes mÃ­nimas:
     - Runtime: **14.3 LTS** ou superior
     - Node type: Menor disponÃ­vel (ex: `i3.xlarge` ou similar)
     - Autoscaling: Desabilitado (1 worker)
3. Aguarde o cluster iniciar (Ã­cone verde)

### 4. Executar o Generator

1. Abra o notebook `databricks-generator.py`
2. Anexe ao cluster criado (dropdown superior direito)
3. Execute **Run All** ou execute cÃ©lula por cÃ©lula:
   - CÃ©lulas 1-7: ConfiguraÃ§Ã£o e funÃ§Ãµes (execute todas)
   - CÃ©lula 8: **Iniciar GeraÃ§Ã£o de Dados** (deixe rodando)

**O que acontece:**
- Gera 1 lote de dados (kafka-orders + kafka-events + kafka-gps) com sequence_id a cada 5 segundos
- Salva como arquivos JSON em `/FileStore/kafka-sim/`
- Cada arquivo JSON contÃ©m um _sequence_id para rastrear a ordem de entrada
- Exibe log de progresso no output da cÃ©lula

**Dica:** Deixe gerando pelo menos 20-30 arquivos antes de iniciar o consumer (1-2 minutos).

### 5. Executar o Consumer

1. Abra o notebook `databricks-consumer.py` **em outra aba**
2. Anexe ao mesmo cluster
3. Execute **Run All** ou execute cÃ©lula por cÃ©lula:
   - CÃ©lulas 1-7: ConfiguraÃ§Ã£o, schemas e criaÃ§Ã£o do database/schema Bronze
   - CÃ©lula 9: Ler streams dos 3 tÃ³picos
   - CÃ©lula 12: **Gravar na Camada Bronze** (inicia o stream contÃ­nuo)

**O que acontece:**
- Auto Loader detecta arquivos JSON automaticamente em `/FileStore/kafka-sim/`
- Multiplexar os 3 streams em um Ãºnico stream unificado
- Transforma dados adicionando metadados do Kafka (key, partition, offset, topic, ingestion_timestamp)
- Grava em tempo real na tabela Delta `bronze.kafka_multiplex` (Hive)
- MantÃ©m checkpoint em `/tmp/kafka_multiplex_checkpoint`
- Processa novos arquivos conforme sÃ£o criados a cada 5 segundos

### 6. Consultar os Dados na Bronze

ApÃ³s o consumer iniciar, vocÃª pode consultar a camada Bronze:

```sql
SELECT topic, COUNT(*) as total, MIN(key) as min_key, MAX(key) as max_key
FROM bronze.kafka_multiplex
GROUP BY topic
```

- **Visualize os dados** usando as queries SQL fornecidas no notebook
- **Monitore progresso** na cÃ©lula 13 que mostra contadores em tempo real
- **Analise por tÃ³pico, partiÃ§Ã£o e offset** usando as queries na cÃ©lula 14

### 7. AnÃ¡lises na Camada Bronze (Hive)

ApÃ³s os dados serem ingeridos, vocÃª pode executar queries SQL:

1. **Contar registros por tÃ³pico:**
   ```sql
   SELECT topic, COUNT(*) as count FROM bronze.kafka_multiplex GROUP BY topic
   ```

2. **DistribuiÃ§Ã£o por partiÃ§Ã£o:**
   ```sql
   SELECT topic, partition, COUNT(*) as count FROM bronze.kafka_multiplex GROUP BY topic, partition
   ```

3. **Verificar offsets:**
   ```sql
   SELECT topic, partition, MAX(offset) as max_offset FROM bronze.kafka_multiplex GROUP BY topic, partition
   ```

4. **Extrair dados do JSON (value):**
   ```sql
   SELECT key, topic, from_json(value, 'STRUCT<data:STRING>').data as data FROM bronze.kafka_multiplex LIMIT 10
   ```

## ComparaÃ§Ã£o: Kafka Local vs Databricks Simulator

| Aspecto | Kafka Local | Databricks Simulator |
|---------|-------------|----------------------|
| **ConfiguraÃ§Ã£o** | Docker + Bore tunnel | Apenas notebooks |
| **LatÃªncia** | Rede externa | DBFS (interno) |
| **Streaming Real** | âœ… Kafka nativo | âš ï¸ Simulado (Auto Loader) |
| **Schemas** | âœ… IdÃªnticos | âœ… IdÃªnticos |
| **Dados** | âœ… IdÃªnticos | âœ… IdÃªnticos |
| **Camada Bronze** | âŒ NÃ£o incluÃ­da | âœ… Medallion Architecture |
| **Melhor para** | ProduÃ§Ã£o/Testes reais | Estudos/Medallion/Prototipagem |

## Comandos Ãšteis

### Verificar Arquivos Gerados

```python
# No Databricks
dbutils.fs.ls("/FileStore/kafka-sim/kafka-orders")
```

### Limpar Dados Antigos

```python
# Limpar arquivos JSON
dbutils.fs.rm("/FileStore/kafka-sim", recurse=True)

# Limpar checkpoints (para reprocessar)
dbutils.fs.rm("/tmp/kafka-sim-checkpoints", recurse=True)
```

### Parar Todos os Streams

```python
# Executar no consumer notebook
for stream in spark.streams.active:
    stream.stop()
```

### Ver Streams Ativos

```python
for stream in spark.streams.active:
    print(f"{stream.name}: {stream.status}")
```

## ConfiguraÃ§Ãµes AvanÃ§adas

### Ajustar Intervalo de GeraÃ§Ã£o

No `databricks-generator.py`, cÃ©lula "ConfiguraÃ§Ãµes":

```python
INTERVAL_SECONDS = 2  # Gera a cada 2 segundos (mais rÃ¡pido)
MAX_FILES = 50        # Limita a 50 arquivos por tÃ³pico
```

### Ajustar Trigger do Stream

No `databricks-consumer.py`, cÃ©lula "ConfiguraÃ§Ãµes":

```python
TRIGGER_INTERVAL = "10 seconds"  # Processa a cada 10 segundos
```

### Salvar em Delta Lake

No `databricks-consumer.py`, descomente a cÃ©lula 13.1:

```python
query_save_ordem = df_ordem \
    .writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/tmp/kafka-sim-checkpoints/save_kafka_orders") \
    .table("kafka_sim_orders")
```

Depois consulte:

```sql
SELECT * FROM kafka_sim_orders LIMIT 10
```

## Exemplos de AnÃ¡lises

### 1. Pedidos por MÃ©todo de Pagamento

```python
display(
    df_ordem
    .groupBy("method")
    .count()
    .orderBy("count", ascending=False)
)
```

### 2. Valor MÃ©dio por Status

```python
from pyspark.sql.functions import avg, round

display(
    df_ordem
    .groupBy("status")
    .agg(
        round(avg("amount"), 2).alias("avg_amount"),
        count("*").alias("count")
    )
)
```

### 3. Eventos em SequÃªncia

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window = Window.partitionBy("payment_id").orderBy("event_timestamp")

display(
    df_eventos
    .withColumn("seq", row_number().over(window))
    .select("payment_id", "event_name", "seq", "event_timestamp")
)
```

### 4. Mapa de Calor GPS

```python
# No display, escolha visualizaÃ§Ã£o "Map" e configure:
# - Latitude: lat
# - Longitude: lon
# - Keys: speed_kph

display(
    df_gps.select("lat", "lon", "speed_kph")
)
```

## Troubleshooting

### Problema: "No files found in path"

**Causa:** O generator ainda nÃ£o criou arquivos.

**SoluÃ§Ã£o:**
1. Verifique se o generator estÃ¡ rodando (cÃ©lula 8)
2. Aguarde pelo menos 30 segundos
3. Execute: `dbutils.fs.ls("/FileStore/kafka-sim/kafka-orders")`

### Problema: Stream nÃ£o atualiza

**Causa:** Checkpoint pode estar travado ou trigger muito longo.

**SoluÃ§Ã£o:**
```python
# Limpar checkpoint
dbutils.fs.rm("/tmp/kafka-sim-checkpoints", recurse=True)

# Reiniciar o consumer
```

### Problema: "Schema mismatch"

**Causa:** Dados antigos com estrutura diferente.

**SoluÃ§Ã£o:**
```python
# Limpar dados antigos
dbutils.fs.rm("/FileStore/kafka-sim", recurse=True)

# Limpar schema checkpoint
dbutils.fs.rm("/tmp/kafka-sim-checkpoints", recurse=True)

# Reiniciar generator e consumer
```

### Problema: Cluster desconectou

**Causa:** Inatividade (Community Edition desconecta apÃ³s 2h).

**SoluÃ§Ã£o:**
1. Reinicie o cluster em **Compute**
2. Re-execute os notebooks do inÃ­cio
3. Checkpoints preservam o progresso

### Problema: "Faker module not found"

**Causa:** Biblioteca nÃ£o instalada no cluster.

**SoluÃ§Ã£o:**
- A cÃ©lula `%pip install faker` no generator instala automaticamente
- Se persistir, reinicie o kernel: `dbutils.library.restartPython()`

## PrÃ³ximos Passos

ApÃ³s dominar este simulador, vocÃª pode:

1. **Criar transformaÃ§Ãµes customizadas** nos streams
2. **Salvar dados processados** em Delta Lake
3. **Criar dashboards** com agregaÃ§Ãµes em tempo real
4. **Testar watermarks** para late data handling
5. **Implementar window operations** para anÃ¡lises temporais
6. **Migrar para Kafka real** quando estiver pronto (o cÃ³digo Ã© muito similar)

## Recursos Adicionais

- [Databricks Auto Loader Docs](https://docs.databricks.com/ingestion/auto-loader/index.html)
- [Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Delta Lake Streaming](https://docs.databricks.com/delta/delta-streaming.html)

## DiferenÃ§as vs. Kafka Real

Para referÃªncia, estas sÃ£o as principais diferenÃ§as ao migrar para Kafka:

### No Gerador:
```python
# Simulador (DBFS)
dbutils.fs.put(filename, json_data)

# Kafka Real
producer.send(topic, value=data, key=key)
```

### No Consumer:
```python
# Simulador (Auto Loader)
.readStream.format("cloudFiles").option("cloudFiles.format", "json")

# Kafka Real
.readStream.format("kafka").option("kafka.bootstrap.servers", "bore.pub:xxxx")
```

O **schema e lÃ³gica de processamento** sÃ£o idÃªnticos! ğŸ‰

---

## Suporte

Problemas ou dÃºvidas:
- Consulte a seÃ§Ã£o **Troubleshooting** acima
- Revise os comentÃ¡rios nos notebooks
- Verifique logs de erro nas cÃ©lulas

**Bons estudos!** ğŸš€
